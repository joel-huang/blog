<!DOCTYPE html><html lang="en" class="h-full w-full !cursor-default __className_d65c78"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/d0e0753be54cb657.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/8c26d386e0f6a889.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/cffd52bf4f3a8ac2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-c852d0775335dfbd.js"/><script src="/_next/static/chunks/7f586deb-8da4bb91273e2b93.js" async=""></script><script src="/_next/static/chunks/104-1f6fea7e46d45122.js" async=""></script><script src="/_next/static/chunks/main-app-bbb6cde570f9e5ff.js" async=""></script><script src="/_next/static/chunks/522-7faf047ae04d7f40.js" async=""></script><script src="/_next/static/chunks/29-11b52f2eb927595e.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-58e2324a29bacae2.js" async=""></script><title>Blazing Fast Pairwise Cosine Similarity | Joel Huang</title><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="Blazing Fast Pairwise Cosine Similarity"/><meta property="og:url" content="https://joelhuang.dev/blog/blazing-fast-pairwise-cosine-similarity"/><meta property="og:image" content="https://joelhuang.dev/og?title=Blazing%20Fast%20Pairwise%20Cosine%20Similarity"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2021-12-18"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Blazing Fast Pairwise Cosine Similarity"/><meta name="twitter:image" content="https://joelhuang.dev/og?title=Blazing%20Fast%20Pairwise%20Cosine%20Similarity"/><link rel="icon" href="/icon.ico?76fc8b9a30e3efac" type="image/x-icon" sizes="16x16"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased max-w-xl mx-4 mt-8 sm:mx-auto"><main class="flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0"><aside class="-ml-[8px] mb-16 tracking-tight"><div class="lg:sticky lg:top-20"><nav class="flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative" id="nav"><div class="flex flex-row space-x-0 pr-10"><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/">about</a><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/blog">blog</a></div></nav></div></aside><section><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","headline":"Blazing Fast Pairwise Cosine Similarity","datePublished":"2021-12-18","dateModified":"2021-12-18","image":"/og?title=Blazing%20Fast%20Pairwise%20Cosine%20Similarity","url":"https://joelhuang.dev/blog/blazing-fast-pairwise-cosine-similarity","author":{"@type":"Person","name":"Joel Huang"}}</script><h1 class="title font-semibold text-2xl tracking-tighter">Blazing Fast Pairwise Cosine Similarity</h1><div class="flex justify-between items-center mt-2 mb-8 text-sm"><p class="text-sm text-neutral-600 dark:text-neutral-400">December 18, 2021</p></div><article class="prose"><h1 id="i-accidentally-implemented-the-fastest-pairwise-cosine-similarity-function">I accidentally implemented the fastest pairwise cosine similarity function</h1>
<!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$-->
<p>While searching for a way to efficiently compute pairwise cosine similarity between vectors, I created a simple and efficient implementation using PyTorch. The function runs blazingly fast. It is faster than the popular <code class="__className_262b13">cosine_similarity</code> function from <code class="__className_262b13">sklearn</code> and the naive loop-based implementations.</p>
<div class="relative"><pre><code class="__className_262b13">def pairwise_cosine_similarity(tensor: Tensor) -> Tensor:
    """
    Args:
        tensor: A tensor of shape (N, D) where N is the number of
        vectors and D is the dimensionality of the vectors.
    Returns:
        A tensor of shape (N, N) containing the cosine similarity
        between all pairs of vectors.
    """

    tmm = torch.mm(tensor, tensor.T)
    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)
    denom_mat = torch.mm(denom.T, denom)
    return torch.nan_to_num(tmm / denom_mat)
</code></pre><div class="absolute top-2 right-2"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>
<h2 id="about-cosine-similarity">About cosine similarity</h2>
<p>Cosine similarity is a intuitive metric to measure similarity between two vectors. It is widely used in vision, recommendation systems, search engines, and natural language processing. The cosine similarity between two vectors is defined as the cosine of the angle between them, ranging from -1 to 1, where 1 means the vectors are identical, -1 means they are opposite, and 0 means they are orthogonal.</p>
<p>Edit (March 2024): Be cautious about using cosine similarity when working with embeddings. Please check out <a target="_blank" rel="noopener noreferrer" class="w-fit h-2 inline-block items-center" href="https://arxiv.org/pdf/2403.05440.pdf"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right inline-block"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg>Is Cosine-Similarity of Embeddings Really About
Similarity?</a></p>
<h2 id="why-pairwise-similarity-is-slow">Why pairwise similarity is slow</h2>
<p>We usually want to compute the cosine similarity between all pairs of vectors. This enables do things like searching for similar vectors (similar images), analyzing the structure of the vector space (clustering images). However, with a large matrix, computing the cosine similarity can be computationally expensive.</p>
<p>We often find ourselves having a matrix of shape <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">)</span></span></span></span> where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span></span></span></span> is the number of vectors and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span> is the dimensionality of the vectors. <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.02778em">D</span></span></span></span> is also called the embedding size or dimension, which is typically a power of 2, e.g. 64, 512, 1024, 4096, etc.</p>
<h2 id="the-method">The method</h2>
<p>The method is based on the following formula:</p>
<center><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>sim</mtext><mo stretchy="false">(</mo><msub><mi>v</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><msub><mi>v</mi><mi>i</mi></msub><mo>⋅</mo><msub><mi>v</mi><mi>j</mi></msub></mrow><mrow><mo stretchy="false">∥</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">∥</mo><mo stretchy="false">∥</mo><msub><mi>v</mi><mi>j</mi></msub><mo stretchy="false">∥</mo></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">\text{sim}(v_i, v_j) = \displaystyle\frac{v_i \cdot v_j}{\lVert v_i \rVert \lVert v_j \rVert}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em"></span><span class="mord text"><span class="mord">sim</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.0936em;vertical-align:-0.9721em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1215em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mopen">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">∥</span><span class="mopen">∥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span><span class="mclose">∥</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9721em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p></center>
<p>Let&#x27;s explain what the code does:</p>
<div class="relative"><pre><code class="__className_262b13">tmm = torch.mm(tensor, tensor.T)
denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)
denom_mat = torch.mm(denom.T, denom)
return torch.nan_to_num(tmm / denom_mat)
</code></pre><div class="absolute top-2 right-2"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>
<ol>
<li>
<p>Numerator matrix: Dot product via matrix multiplication</p>
<p><code class="__className_262b13">tmm = torch.mm(tensor, tensor.T)</code></p>
<blockquote>
<p>This line computes the matrix multiplication of the input <code class="__className_262b13">tensor</code> with its transpose <code class="__className_262b13">tensor.T</code>. The result is a tensor <code class="__className_262b13">tmm</code> of shape <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mclose">)</span></span></span></span> where each element <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i, j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mclose">)</span></span></span></span> represents the dot product of vectors <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span>.</p>
</blockquote>
</li>
<li>
<p>Denominator values: Norm of each vector</p>
<p><code class="__className_262b13">denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)</code></p>
<blockquote>
<p>The diagonal of the tensor <code class="__className_262b13">tmm</code> contains the dot products of each vector with itself. Taking the square root of these values gives the magnitude (or norm) of each vector. The <code class="__className_262b13">unsqueeze(0)</code> function is used to add an extra dimension to the tensor, changing its shape from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N,)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mclose">)</span></span></span></span> to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mn>1</mn><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1, N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mclose">)</span></span></span></span>.</p>
</blockquote>
</li>
<li>
<p>Denominator matrix</p>
<p><code class="__className_262b13">denom_mat = torch.mm(denom.T, denom)</code></p>
<blockquote>
<p>This line computes the outer product of the vector <code class="__className_262b13">denom</code> with itself, resulting in a matrix <code class="__className_262b13">denom_mat</code> of shape <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, N)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mclose">)</span></span></span></span>. Each element <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i, j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mclose">)</span></span></span></span> of this matrix is the product of the magnitudes of vectors <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span>.</p>
</blockquote>
</li>
<li>
<p>NaN removal</p>
<p><code class="__className_262b13">return torch.nan_to_num(tmm / denom_mat)</code></p>
<blockquote>
<p>Finally, the cosine similarity between each pair of vectors is calculated by dividing the dot product matrix <code class="__className_262b13">tmm</code> by the matrix <code class="__className_262b13">denom_mat</code>. The division is element-wise, so each element <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(i, j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span><span class="mclose">)</span></span></span></span> of the resulting matrix represents the cosine similarity between vectors <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em"></span><span class="mord mathnormal">i</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span>. <code class="__className_262b13">torch.nan_to_num</code> is used to replace any NaN values that might occur during the division with zeros.</p>
</blockquote>
</li>
</ol>
<p>The output is a symmetric matrix where the diagonal elements are all 1 (since the cosine similarity of a vector with itself is always 1), and the off-diagonal elements represent the cosine similarity between different pairs of vectors!</p>
<h2 id="benchmarking">Benchmarking</h2>
<p>Versus naive loops, our approach completely outperforms them by several orders of magnitude. Versus <code class="__className_262b13">sklearn.metrics.pairwise.cosine_similarity</code>, our implementation is 10x faster, and versus a <code class="__className_262b13">numpy</code> implementation using the exact same logic, our PyTorch code is about 2-3x faster.</p>
<h3 id="results">Results</h3>
<div class="relative"><pre><code class="__className_262b13">Result within 1e-8 of scipy loop: True
Result within 1e-8 of numpy loop: True
Result within 1e-8 of torch loop: True
Result within 1e-8 of sklearn: True
Result within 1e-8 of numpy matrix: True
scipy loop:   142362.0 us
numpy loop:   112752.9 us
torch loop:   83144.2 us
sklearn:      401.8 us
numpy matrix: 136.2 us
✨ ours:      48.6 us
</code></pre><div class="absolute top-2 right-2"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div>
<h3 id="benchmark-code">Benchmark code</h3>
<details closed="true"><summary>Expand to see the benchmark code used to test the functions.</summary><div class="relative"><pre><code class="__className_262b13">import torch
import numpy as np
import scipy
from sklearn.metrics.pairwise import cosine_similarity


def torch_loop_cosine_similarity(tensor, output):
    for i in range(len(tensor)):
        for j in range(len(tensor)):
            output[i, j] = torch.cosine_similarity(
                tensor[i].unsqueeze(0), tensor[j].unsqueeze(0)
            )
    return output


def scipy_loop_cosine_similarity(tensor, output):
    for i in range(len(tensor)):
        for j in range(len(tensor)):
            output[i, j] = scipy.spatial.distance.cosine(tensor[i], tensor[j])
    return output


def numpy_loop_cosine_similarity(tensor, output):
    for i in range(len(tensor)):
        for j in range(len(tensor)):
            output[i, j] = float(
                np.dot(tensor[i], tensor[j])
                / (np.linalg.norm(tensor[i]) * np.linalg.norm(tensor[j]))
            )
    return output


def numpy_matrix_cosine_similarity(tensor):
    tmm = np.matmul(tensor, tensor.T)
    denom = np.sqrt(tmm.diagonal()).unsqueeze(0)
    denom_mat = np.matmul(denom.T, denom)
    return np.nan_to_num(tmm / denom_mat)


def torch_matrix_cosine_similarity(tensor):
    tmm = torch.mm(tensor, tensor.T)
    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)
    denom_mat = torch.mm(denom.T, denom)
    return torch.nan_to_num(tmm / denom_mat)


if __name__ == "__main__":

    # Create a random data tensor of shape (N=50, D=64)
    data = torch.rand((50, 64))

    # Create the output tensor
    zeros = torch.zeros((data.shape[0], data.shape[0]))

    # Compare across different implementations
    sklearn = torch.Tensor(cosine_similarity(data, data))
    npy_matrix = torch.Tensor(numpy_matrix_cosine_similarity(data))
    npy_loop = numpy_loop_cosine_similarity(data, zeros)
    spy_loop = scipy_loop_cosine_similarity(data, zeros)
    torch_loop = torch_loop_cosine_similarity(data, zeros)
    ours = torch_matrix_cosine_similarity(data)

    print(f"Result within 1e-8 of scipy loop: {bool(torch.allclose(ours, spy_loop))}")
    print(f"Result within 1e-8 of numpy loop: {bool(torch.allclose(ours, npy_loop))}")
    print(f"Result within 1e-8 of torch loop: {bool(torch.allclose(ours, torch_loop))}")
    print(f"Result within 1e-8 of sklearn: {bool(torch.allclose(ours, sklearn))}")
    print(
        f"Result within 1e-8 of numpy matrix: {bool(torch.allclose(ours, npy_matrix))}"
    )

    import timeit

    t0 = timeit.Timer(
        stmt="torch_loop_cosine_similarity(data, output)",
        setup="from __main__ import torch_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));",
        globals={"data": data},
    )

    t1 = timeit.Timer(
        stmt="cosine_similarity(data, data)",
        setup="from __main__ import cosine_similarity",
        globals={"data": data},
    )

    t2 = timeit.Timer(
        stmt="scipy_loop_cosine_similarity(data, output)",
        setup="from __main__ import scipy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));",
        globals={"data": data},
    )

    t5 = timeit.Timer(
        stmt="numpy_loop_cosine_similarity(data, output)",
        setup="from __main__ import numpy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));",
        globals={"data": data},
    )

    t3 = timeit.Timer(
        stmt="numpy_matrix_cosine_similarity(data)",
        setup="from __main__ import numpy_matrix_cosine_similarity",
        globals={"data": data},
    )

    t4 = timeit.Timer(
        stmt="torch_matrix_cosine_similarity(data)",
        setup="from __main__ import torch_matrix_cosine_similarity",
        globals={"data": data},
    )

    print(f"scipy loop:   {t2.timeit(100) / 100 * 1e6:>5.1f} us")
    print(f"numpy loop:   {t5.timeit(100) / 100 * 1e6:>5.1f} us")
    print(f"torch loop:   {t0.timeit(100) / 100 * 1e6:>5.1f} us")
    print(f"sklearn:      {t1.timeit(100) / 100 * 1e6:>5.1f} us")
    print(f"numpy matrix: {t3.timeit(100) / 100 * 1e6:>5.1f} us")
    print(f"✨ ours:     {t4.timeit(10) / 100 * 1e6:>5.1f} us")
</code></pre><div class="absolute top-2 right-2"><!--$!--><template data-dgst="BAILOUT_TO_CLIENT_SIDE_RENDERING"></template><!--/$--></div></div></details></article></section><footer class="mb-16"><p class="mt-8 text-neutral-600 dark:text-neutral-600">© <!-- -->2025<!-- --> Joel Huang</p></footer></main><script src="/_next/static/chunks/webpack-c852d0775335dfbd.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/css/d0e0753be54cb657.css\",\"style\"]\n2:HL[\"/_next/static/css/8c26d386e0f6a889.css\",\"style\"]\n3:HL[\"/_next/static/css/cffd52bf4f3a8ac2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"4:I[10242,[],\"\"]\n7:I[48071,[],\"\"]\n9:I[58073,[],\"\"]\na:I[28522,[\"522\",\"static/chunks/522-7faf047ae04d7f40.js\",\"29\",\"static/chunks/29-11b52f2eb927595e.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2324a29bacae2.js\"],\"\"]\nc:I[47832,[],\"\"]\n8:[\"slug\",\"blazing-fast-pairwise-cosine-similarity\",\"d\"]\nd:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L4\",null,{\"buildId\":\"hofQAQpKqopSAnHskrIVg\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"blazing-fast-pairwise-cosine-similarity\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"blazing-fast-pairwise-cosine-similarity\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"blazing-fast-pairwise-cosine-similarity\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"blazing-fast-pairwise-cosine-similarity\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L5\",[\"$\",\"section\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"suppressHydrationWarning\":true,\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BlogPosting\\\",\\\"headline\\\":\\\"Blazing Fast Pairwise Cosine Similarity\\\",\\\"datePublished\\\":\\\"2021-12-18\\\",\\\"dateModified\\\":\\\"2021-12-18\\\",\\\"image\\\":\\\"/og?title=Blazing%20Fast%20Pairwise%20Cosine%20Similarity\\\",\\\"url\\\":\\\"https://joelhuang.dev/blog/blazing-fast-pairwise-cosine-similarity\\\",\\\"author\\\":{\\\"@type\\\":\\\"Person\\\",\\\"name\\\":\\\"Joel Huang\\\"}}\"}}],[\"$\",\"h1\",null,{\"className\":\"title font-semibold text-2xl tracking-tighter\",\"children\":\"Blazing Fast Pairwise Cosine Similarity\"}],[\"$\",\"div\",null,{\"className\":\"flex justify-between items-center mt-2 mb-8 text-sm\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-sm text-neutral-600 dark:text-neutral-400\",\"children\":\"December 18, 2021\"}]}],[\"$\",\"article\",null,{\"className\":\"prose\",\"children\":\"$L6\"}]]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/cffd52bf4f3a8ac2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$8\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/d0e0753be54cb657.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8c26d386e0f6a889.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"h-full w-full !cursor-default __className_d65c78\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased max-w-xl mx-4 mt-8 sm:mx-auto\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"-ml-[8px] mb-16 tracking-tight\",\"children\":[\"$\",\"div\",null,{\"className\":\"lg:sticky lg:top-20\",\"children\":[\"$\",\"nav\",null,{\"className\":\"flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative\",\"id\":\"nav\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-row space-x-0 pr-10\",\"children\":[[\"$\",\"$La\",\"/\",{\"href\":\"/\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"about\"}],[\"$\",\"$La\",\"/blog\",{\"href\":\"/blog\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"blog\"}]]}]}]}]}],[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-8 text-2xl font-semibold tracking-tighter\",\"children\":\"404 - Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The page you are looking for does not exist.\"}]]}],\"notFoundStyles\":[]}],[\"$\",\"footer\",null,{\"className\":\"mb-16\",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-8 text-neutral-600 dark:text-neutral-600\",\"children\":[\"© \",2025,\" Joel Huang\"]}]}]]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lb\"],\"globalErrorComponent\":\"$c\",\"missingSlots\":\"$Wd\"}]\n"])</script><script>self.__next_f.push([1,"e:\"$Sreact.suspense\"\nf:I[84539,[\"522\",\"static/chunks/522-7faf047ae04d7f40.js\",\"29\",\"static/chunks/29-11b52f2eb927595e.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2324a29bacae2.js\"],\"BailoutToCSR\"]\n10:I[29080,[\"522\",\"static/chunks/522-7faf047ae04d7f40.js\",\"29\",\"static/chunks/29-11b52f2eb927595e.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2324a29bacae2.js\"],\"default\"]\n11:I[15791,[\"522\",\"static/chunks/522-7faf047ae04d7f40.js\",\"29\",\"static/chunks/29-11b52f2eb927595e.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-58e2324a29bacae2.js\"],\"CopyCodeButton\",1]\n12:[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"def pairwise_cosine_similarity(tensor: Tensor) -\u003e Tensor:\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n        tensor: A tensor of shape (N, D) where N is the number of\\r\\n        vectors and D is the dimensionality of the vectors.\\r\\n    Returns:\\r\\n        A tensor of shape (N, N) containing the cosine similarity\\r\\n        between all pairs of vectors.\\r\\n    \\\"\\\"\\\"\\r\\n\\r\\n    tmm = torch.mm(tensor, tensor.T)\\r\\n    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\\r\\n    denom_mat = torch.mm(denom.T, denom)\\r\\n    return torch.nan_to_num(tmm / denom_mat)\\n\"},\"className\":\"__className_262b13\"}]\n13:[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tmm = torch.mm(tensor, tensor.T)\\r\\ndenom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\\r\\ndenom_mat = torch.mm(denom.T, denom)\\r\\nreturn torch.nan_to_num(tmm / denom_mat)\\n\"},\"className\":\"__className_262b13\"}]\n14:[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Result within 1e-8 of scipy loop: True\\r\\nResult within 1e-8 of numpy loop: True\\r\\nResult within 1e-8 of torch loop: True\\r\\nResult within 1e-8 of sklearn: True\\r\\nResult within 1e-8 of numpy matrix: True\\r\\nscipy loop:   142362.0 us\\r\\nnumpy loop:   112752.9 us\\r\\ntorch loop:   83144.2 us\\r\\nsklearn:      401.8 us\\r\\nnumpy matrix: 136.2 us\\r\\n✨ ours:      48.6 us\\n\"},\"className\":\"__className_262b13\"}]\n15:T1079,"])</script><script>self.__next_f.push([1,"import torch\r\nimport numpy as np\r\nimport scipy\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\n\r\ndef torch_loop_cosine_similarity(tensor, output):\r\n    for i in range(len(tensor)):\r\n        for j in range(len(tensor)):\r\n            output[i, j] = torch.cosine_similarity(\r\n                tensor[i].unsqueeze(0), tensor[j].unsqueeze(0)\r\n            )\r\n    return output\r\n\r\n\r\ndef scipy_loop_cosine_similarity(tensor, output):\r\n    for i in range(len(tensor)):\r\n        for j in range(len(tensor)):\r\n            output[i, j] = scipy.spatial.distance.cosine(tensor[i], tensor[j])\r\n    return output\r\n\r\n\r\ndef numpy_loop_cosine_similarity(tensor, output):\r\n    for i in range(len(tensor)):\r\n        for j in range(len(tensor)):\r\n            output[i, j] = float(\r\n                np.dot(tensor[i], tensor[j])\r\n                / (np.linalg.norm(tensor[i]) * np.linalg.norm(tensor[j]))\r\n            )\r\n    return output\r\n\r\n\r\ndef numpy_matrix_cosine_similarity(tensor):\r\n    tmm = np.matmul(tensor, tensor.T)\r\n    denom = np.sqrt(tmm.diagonal()).unsqueeze(0)\r\n    denom_mat = np.matmul(denom.T, denom)\r\n    return np.nan_to_num(tmm / denom_mat)\r\n\r\n\r\ndef torch_matrix_cosine_similarity(tensor):\r\n    tmm = torch.mm(tensor, tensor.T)\r\n    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\r\n    denom_mat = torch.mm(denom.T, denom)\r\n    return torch.nan_to_num(tmm / denom_mat)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # Create a random data tensor of shape (N=50, D=64)\r\n    data = torch.rand((50, 64))\r\n\r\n    # Create the output tensor\r\n    zeros = torch.zeros((data.shape[0], data.shape[0]))\r\n\r\n    # Compare across different implementations\r\n    sklearn = torch.Tensor(cosine_similarity(data, data))\r\n    npy_matrix = torch.Tensor(numpy_matrix_cosine_similarity(data))\r\n    npy_loop = numpy_loop_cosine_similarity(data, zeros)\r\n    spy_loop = scipy_loop_cosine_similarity(data, zeros)\r\n    torch_loop = torch_loop_cosine_similarity(data, zeros)\r\n    ours = torch_matrix_cosine_similarity(data)\r\n\r\n    print(f\"Result within 1e-8 of scipy loop: {bool(torch.allclose(ours, spy_loop))}\")\r\n    print(f\"Result within 1e-8 of numpy loop: {bool(torch.allclose(ours, npy_loop))}\")\r\n    print(f\"Result within 1e-8 of torch loop: {bool(torch.allclose(ours, torch_loop))}\")\r\n    print(f\"Result within 1e-8 of sklearn: {bool(torch.allclose(ours, sklearn))}\")\r\n    print(\r\n        f\"Result within 1e-8 of numpy matrix: {bool(torch.allclose(ours, npy_matrix))}\"\r\n    )\r\n\r\n    import timeit\r\n\r\n    t0 = timeit.Timer(\r\n        stmt=\"torch_loop_cosine_similarity(data, output)\",\r\n        setup=\"from __main__ import torch_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t1 = timeit.Timer(\r\n        stmt=\"cosine_similarity(data, data)\",\r\n        setup=\"from __main__ import cosine_similarity\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t2 = timeit.Timer(\r\n        stmt=\"scipy_loop_cosine_similarity(data, output)\",\r\n        setup=\"from __main__ import scipy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t5 = timeit.Timer(\r\n        stmt=\"numpy_loop_cosine_similarity(data, output)\",\r\n        setup=\"from __main__ import numpy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t3 = timeit.Timer(\r\n        stmt=\"numpy_matrix_cosine_similarity(data)\",\r\n        setup=\"from __main__ import numpy_matrix_cosine_similarity\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t4 = timeit.Timer(\r\n        stmt=\"torch_matrix_cosine_similarity(data)\",\r\n        setup=\"from __main__ import torch_matrix_cosine_similarity\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    print(f\"scipy loop:   {t2.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"numpy loop:   {t5.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"torch loop:   {t0.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"sklearn:      {t1.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"numpy matrix: {t3.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"✨ ours:     {t4.timeit(10) / 100 * 1e6:\u003e5.1f} us\")\n"])</script><script>self.__next_f.push([1,"17:T1079,"])</script><script>self.__next_f.push([1,"import torch\r\nimport numpy as np\r\nimport scipy\r\nfrom sklearn.metrics.pairwise import cosine_similarity\r\n\r\n\r\ndef torch_loop_cosine_similarity(tensor, output):\r\n    for i in range(len(tensor)):\r\n        for j in range(len(tensor)):\r\n            output[i, j] = torch.cosine_similarity(\r\n                tensor[i].unsqueeze(0), tensor[j].unsqueeze(0)\r\n            )\r\n    return output\r\n\r\n\r\ndef scipy_loop_cosine_similarity(tensor, output):\r\n    for i in range(len(tensor)):\r\n        for j in range(len(tensor)):\r\n            output[i, j] = scipy.spatial.distance.cosine(tensor[i], tensor[j])\r\n    return output\r\n\r\n\r\ndef numpy_loop_cosine_similarity(tensor, output):\r\n    for i in range(len(tensor)):\r\n        for j in range(len(tensor)):\r\n            output[i, j] = float(\r\n                np.dot(tensor[i], tensor[j])\r\n                / (np.linalg.norm(tensor[i]) * np.linalg.norm(tensor[j]))\r\n            )\r\n    return output\r\n\r\n\r\ndef numpy_matrix_cosine_similarity(tensor):\r\n    tmm = np.matmul(tensor, tensor.T)\r\n    denom = np.sqrt(tmm.diagonal()).unsqueeze(0)\r\n    denom_mat = np.matmul(denom.T, denom)\r\n    return np.nan_to_num(tmm / denom_mat)\r\n\r\n\r\ndef torch_matrix_cosine_similarity(tensor):\r\n    tmm = torch.mm(tensor, tensor.T)\r\n    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\r\n    denom_mat = torch.mm(denom.T, denom)\r\n    return torch.nan_to_num(tmm / denom_mat)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # Create a random data tensor of shape (N=50, D=64)\r\n    data = torch.rand((50, 64))\r\n\r\n    # Create the output tensor\r\n    zeros = torch.zeros((data.shape[0], data.shape[0]))\r\n\r\n    # Compare across different implementations\r\n    sklearn = torch.Tensor(cosine_similarity(data, data))\r\n    npy_matrix = torch.Tensor(numpy_matrix_cosine_similarity(data))\r\n    npy_loop = numpy_loop_cosine_similarity(data, zeros)\r\n    spy_loop = scipy_loop_cosine_similarity(data, zeros)\r\n    torch_loop = torch_loop_cosine_similarity(data, zeros)\r\n    ours = torch_matrix_cosine_similarity(data)\r\n\r\n    print(f\"Result within 1e-8 of scipy loop: {bool(torch.allclose(ours, spy_loop))}\")\r\n    print(f\"Result within 1e-8 of numpy loop: {bool(torch.allclose(ours, npy_loop))}\")\r\n    print(f\"Result within 1e-8 of torch loop: {bool(torch.allclose(ours, torch_loop))}\")\r\n    print(f\"Result within 1e-8 of sklearn: {bool(torch.allclose(ours, sklearn))}\")\r\n    print(\r\n        f\"Result within 1e-8 of numpy matrix: {bool(torch.allclose(ours, npy_matrix))}\"\r\n    )\r\n\r\n    import timeit\r\n\r\n    t0 = timeit.Timer(\r\n        stmt=\"torch_loop_cosine_similarity(data, output)\",\r\n        setup=\"from __main__ import torch_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t1 = timeit.Timer(\r\n        stmt=\"cosine_similarity(data, data)\",\r\n        setup=\"from __main__ import cosine_similarity\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t2 = timeit.Timer(\r\n        stmt=\"scipy_loop_cosine_similarity(data, output)\",\r\n        setup=\"from __main__ import scipy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t5 = timeit.Timer(\r\n        stmt=\"numpy_loop_cosine_similarity(data, output)\",\r\n        setup=\"from __main__ import numpy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t3 = timeit.Timer(\r\n        stmt=\"numpy_matrix_cosine_similarity(data)\",\r\n        setup=\"from __main__ import numpy_matrix_cosine_similarity\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    t4 = timeit.Timer(\r\n        stmt=\"torch_matrix_cosine_similarity(data)\",\r\n        setup=\"from __main__ import torch_matrix_cosine_similarity\",\r\n        globals={\"data\": data},\r\n    )\r\n\r\n    print(f\"scipy loop:   {t2.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"numpy loop:   {t5.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"torch loop:   {t0.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"sklearn:      {t1.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"numpy matrix: {t3.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\r\n    print(f\"✨ ours:     {t4.timeit(10) / 100 * 1e6:\u003e5.1f} us\")\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$17\"},\"className\":\"__className_262b13\"}]\n"])</script><script>self.__next_f.push([1,"6:[[\"$\",\"h1\",null,{\"id\":\"i-accidentally-implemented-the-fastest-pairwise-cosine-similarity-function\",\"children\":\"I accidentally implemented the fastest pairwise cosine similarity function\"}],\"\\n\",[\"$\",\"$e\",null,{\"fallback\":null,\"children\":[\"$\",\"$Lf\",null,{\"reason\":\"next/dynamic\",\"children\":[\"$\",\"$L10\",null,{}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"While searching for a way to efficiently compute pairwise cosine similarity between vectors, I created a simple and efficient implementation using PyTorch. The function runs blazingly fast. It is faster than the popular \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"cosine_similarity\"},\"className\":\"__className_262b13\"}],\" function from \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"sklearn\"},\"className\":\"__className_262b13\"}],\" and the naive loop-based implementations.\"]}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"def pairwise_cosine_similarity(tensor: Tensor) -\u003e Tensor:\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n        tensor: A tensor of shape (N, D) where N is the number of\\r\\n        vectors and D is the dimensionality of the vectors.\\r\\n    Returns:\\r\\n        A tensor of shape (N, N) containing the cosine similarity\\r\\n        between all pairs of vectors.\\r\\n    \\\"\\\"\\\"\\r\\n\\r\\n    tmm = torch.mm(tensor, tensor.T)\\r\\n    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\\r\\n    denom_mat = torch.mm(denom.T, denom)\\r\\n    return torch.nan_to_num(tmm / denom_mat)\\n\"},\"className\":\"__className_262b13\"}]}],[\"$\",\"div\",null,{\"className\":\"absolute top-2 right-2\",\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":[\"$\",\"$Lf\",null,{\"reason\":\"next/dynamic\",\"children\":[\"$\",\"$L11\",null,{\"pre\":\"$12\"}]}]}]}]]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"about-cosine-similarity\",\"children\":\"About cosine similarity\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Cosine similarity is a intuitive metric to measure similarity between two vectors. It is widely used in vision, recommendation systems, search engines, and natural language processing. The cosine similarity between two vectors is defined as the cosine of the angle between them, ranging from -1 to 1, where 1 means the vectors are identical, -1 means they are opposite, and 0 means they are orthogonal.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Edit (March 2024): Be cautious about using cosine similarity when working with embeddings. Please check out \",[\"$\",\"$La\",null,{\"href\":\"https://arxiv.org/pdf/2403.05440.pdf\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"w-fit h-2 inline-block items-center\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":16,\"height\":16,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right inline-block\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}],\"Is Cosine-Similarity of Embeddings Really About\\r\\nSimilarity?\"]}]]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"why-pairwise-similarity-is-slow\",\"children\":\"Why pairwise similarity is slow\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We usually want to compute the cosine similarity between all pairs of vectors. This enables do things like searching for similar vectors (similar images), analyzing the structure of the vector space (clustering images). However, with a large matrix, computing the cosine similarity can be computationally expensive.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We often find ourselves having a matrix of shape \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"D\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(N, D)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"D\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" where \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"N\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"N\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}]]}]}]]}],\" is the number of vectors and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"D\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"D\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"D\"}]]}]}]]}],\" is the dimensionality of the vectors. \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"D\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"D\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6833em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.02778em\"},\"children\":\"D\"}]]}]}]]}],\" is also called the embedding size or dimension, which is typically a power of 2, e.g. 64, 512, 1024, 4096, etc.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"the-method\",\"children\":\"The method\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The method is based on the following formula:\"}],\"\\n\",[\"$\",\"center\",null,{\"children\":[\"$\",\"p\",null,{\"children\":[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mtext\",null,{\"children\":\"sim\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"j\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}],[\"$\",\"mo\",null,{\"children\":\"=\"}],[\"$\",\"mstyle\",null,{\"scriptlevel\":\"0\",\"displaystyle\":\"true\",\"children\":[\"$\",\"mfrac\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"mo\",null,{\"children\":\"⋅\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"j\"}]]}]]}],[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"∥\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"∥\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"∥\"}],[\"$\",\"msub\",null,{\"children\":[[\"$\",\"mi\",null,{\"children\":\"v\"}],[\"$\",\"mi\",null,{\"children\":\"j\"}]]}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"∥\"}]]}]]}]}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"\\\\text{sim}(v_i, v_j) = \\\\displaystyle\\\\frac{v_i \\\\cdot v_j}{\\\\lVert v_i \\\\rVert \\\\lVert v_j \\\\rVert}\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1.0361em\",\"verticalAlign\":\"-0.2861em\"}}],[\"$\",\"span\",null,{\"className\":\"mord text\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"sim\"}]}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0359em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0359em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}],[\"$\",\"span\",null,{\"className\":\"mrel\",\"children\":\"=\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2778em\"}}]]}],[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"2.0936em\",\"verticalAlign\":\"-0.9721em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mopen nulldelimiter\"}],[\"$\",\"span\",null,{\"className\":\"mfrac\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"1.1215em\"},\"children\":[[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.314em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0359em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"∥\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0359em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\"∥\"}]]}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.23em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"frac-line\",\"style\":{\"borderBottomWidth\":\"0.04em\"}}]]}],[\"$\",\"span\",null,{\"style\":{\"top\":\"-3.677em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"3em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0359em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"children\":\"i\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.15em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mbin\",\"children\":\"⋅\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.2222em\"}}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":[[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.03588em\"},\"children\":\"v\"}],[\"$\",\"span\",null,{\"className\":\"msupsub\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist-t vlist-t2\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.3117em\"},\"children\":[\"$\",\"span\",null,{\"style\":{\"top\":\"-2.55em\",\"marginLeft\":\"-0.0359em\",\"marginRight\":\"0.05em\"},\"children\":[[\"$\",\"span\",null,{\"className\":\"pstrut\",\"style\":{\"height\":\"2.7em\"}}],[\"$\",\"span\",null,{\"className\":\"sizing reset-size6 size3 mtight\",\"children\":[\"$\",\"span\",null,{\"className\":\"mord mathnormal mtight\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.2861em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}]]}]]}]]}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-s\",\"children\":\"​\"}]]}],[\"$\",\"span\",null,{\"className\":\"vlist-r\",\"children\":[\"$\",\"span\",null,{\"className\":\"vlist\",\"style\":{\"height\":\"0.9721em\"},\"children\":[\"$\",\"span\",null,{}]}]}]]}]}],[\"$\",\"span\",null,{\"className\":\"mclose nulldelimiter\"}]]}]]}]]}]]}]}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Let's explain what the code does:\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tmm = torch.mm(tensor, tensor.T)\\r\\ndenom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\\r\\ndenom_mat = torch.mm(denom.T, denom)\\r\\nreturn torch.nan_to_num(tmm / denom_mat)\\n\"},\"className\":\"__className_262b13\"}]}],[\"$\",\"div\",null,{\"className\":\"absolute top-2 right-2\",\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":[\"$\",\"$Lf\",null,{\"reason\":\"next/dynamic\",\"children\":[\"$\",\"$L11\",null,{\"pre\":\"$13\"}]}]}]}]]}],\"\\n\",[\"$\",\"ol\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Numerator matrix: Dot product via matrix multiplication\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tmm = torch.mm(tensor, tensor.T)\"},\"className\":\"__className_262b13\"}]}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This line computes the matrix multiplication of the input \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tensor\"},\"className\":\"__className_262b13\"}],\" with its transpose \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tensor.T\"},\"className\":\"__className_262b13\"}],\". The result is a tensor \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tmm\"},\"className\":\"__className_262b13\"}],\" of shape \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(N, N)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" where each element \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"j\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(i, j)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" represents the dot product of vectors \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"i\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"i\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6595em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"i\"}]]}]}]]}],\" and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"j\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"j\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.854em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}]]}]}]]}],\".\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Denominator values: Norm of each vector\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\"},\"className\":\"__className_262b13\"}]}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The diagonal of the tensor \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tmm\"},\"className\":\"__className_262b13\"}],\" contains the dot products of each vector with itself. Taking the square root of these values gives the magnitude (or norm) of each vector. The \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"unsqueeze(0)\"},\"className\":\"__className_262b13\"}],\" function is used to add an extra dimension to the tensor, changing its shape from \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(N,)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" to \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mn\",null,{\"children\":\"1\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(1, N)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord\",\"children\":\"1\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\".\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"Denominator matrix\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"denom_mat = torch.mm(denom.T, denom)\"},\"className\":\"__className_262b13\"}]}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"This line computes the outer product of the vector \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"denom\"},\"className\":\"__className_262b13\"}],\" with itself, resulting in a matrix \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"denom_mat\"},\"className\":\"__className_262b13\"}],\" of shape \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"N\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(N, N)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.10903em\"},\"children\":\"N\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\". Each element \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"j\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(i, j)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" of this matrix is the product of the magnitudes of vectors \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"i\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"i\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6595em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"i\"}]]}]}]]}],\" and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"j\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"j\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.854em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}]]}]}]]}],\".\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":\"NaN removal\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"return torch.nan_to_num(tmm / denom_mat)\"},\"className\":\"__className_262b13\"}]}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Finally, the cosine similarity between each pair of vectors is calculated by dividing the dot product matrix \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"tmm\"},\"className\":\"__className_262b13\"}],\" by the matrix \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"denom_mat\"},\"className\":\"__className_262b13\"}],\". The division is element-wise, so each element \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\"(\"}],[\"$\",\"mi\",null,{\"children\":\"i\"}],[\"$\",\"mo\",null,{\"separator\":\"true\",\"children\":\",\"}],[\"$\",\"mi\",null,{\"children\":\"j\"}],[\"$\",\"mo\",null,{\"stretchy\":\"false\",\"children\":\")\"}]]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"(i, j)\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"1em\",\"verticalAlign\":\"-0.25em\"}}],[\"$\",\"span\",null,{\"className\":\"mopen\",\"children\":\"(\"}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"i\"}],[\"$\",\"span\",null,{\"className\":\"mpunct\",\"children\":\",\"}],[\"$\",\"span\",null,{\"className\":\"mspace\",\"style\":{\"marginRight\":\"0.1667em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}],[\"$\",\"span\",null,{\"className\":\"mclose\",\"children\":\")\"}]]}]}]]}],\" of the resulting matrix represents the cosine similarity between vectors \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"i\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"i\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.6595em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"children\":\"i\"}]]}]}]]}],\" and \",[\"$\",\"span\",null,{\"className\":\"katex\",\"children\":[[\"$\",\"span\",null,{\"className\":\"katex-mathml\",\"children\":[\"$\",\"math\",null,{\"xmlns\":\"http://www.w3.org/1998/Math/MathML\",\"children\":[\"$\",\"semantics\",null,{\"children\":[[\"$\",\"mrow\",null,{\"children\":[\"$\",\"mi\",null,{\"children\":\"j\"}]}],[\"$\",\"annotation\",null,{\"encoding\":\"application/x-tex\",\"children\":\"j\"}]]}]}]}],[\"$\",\"span\",null,{\"className\":\"katex-html\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"span\",null,{\"className\":\"base\",\"children\":[[\"$\",\"span\",null,{\"className\":\"strut\",\"style\":{\"height\":\"0.854em\",\"verticalAlign\":\"-0.1944em\"}}],[\"$\",\"span\",null,{\"className\":\"mord mathnormal\",\"style\":{\"marginRight\":\"0.05724em\"},\"children\":\"j\"}]]}]}]]}],\". \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"torch.nan_to_num\"},\"className\":\"__className_262b13\"}],\" is used to replace any NaN values that might occur during the division with zeros.\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The output is a symmetric matrix where the diagonal elements are all 1 (since the cosine similarity of a vector with itself is always 1), and the off-diagonal elements represent the cosine similarity between different pairs of vectors!\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"benchmarking\",\"children\":\"Benchmarking\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Versus naive loops, our approach completely outperforms them by several orders of magnitude. Versus \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"sklearn.metrics.pairwise.cosine_similarity\"},\"className\":\"__className_262b13\"}],\", our implementation is 10x faster, and versus a \",[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"numpy\"},\"className\":\"__className_262b13\"}],\" implementation using the exact same logic, our PyTorch code is about 2-3x faster.\"]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"results\",\"children\":\"Results\"}],\"\\n\",[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"Result within 1e-8 of scipy loop: True\\r\\nResult within 1e-8 of numpy loop: True\\r\\nResult within 1e-8 of torch loop: True\\r\\nResult within 1e-8 of sklearn: True\\r\\nResult within 1e-8 of numpy matrix: True\\r\\nscipy loop:   142362.0 us\\r\\nnumpy loop:   112752.9 us\\r\\ntorch loop:   83144.2 us\\r\\nsklearn:      401.8 us\\r\\nnumpy matrix: 136.2 us\\r\\n✨ ours:      48.6 us\\n\"},\"className\":\"__className_262b13\"}]}],[\"$\",\"div\",null,{\"className\":\"absolute top-2 right-2\",\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":[\"$\",\"$Lf\",null,{\"reason\":\"next/dynamic\",\"children\":[\"$\",\"$L11\",null,{\"pre\":\"$14\"}]}]}]}]]}],\"\\n\",[\"$\",\"h3\",null,{\"id\":\"benchmark-code\",\"children\":\"Benchmark code\"}],\"\\n\",[\"$\",\"details\",null,{\"closed\":\"true\",\"children\":[[\"$\",\"summary\",null,{\"children\":\"Expand to see the benchmark code used to test the functions.\"}],[\"$\",\"div\",null,{\"className\":\"relative\",\"children\":[[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$15\"},\"className\":\"__className_262b13\"}]}],[\"$\",\"div\",null,{\"className\":\"absolute top-2 right-2\",\"children\":[\"$\",\"$e\",null,{\"fallback\":null,\"children\":[\"$\",\"$Lf\",null,{\"reason\":\"next/dynamic\",\"children\":[\"$\",\"$L11\",null,{\"pre\":\"$16\"}]}]}]}]]}]]}]]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Blazing Fast Pairwise Cosine Similarity | Joel Huang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"4\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"5\",{\"property\":\"og:title\",\"content\":\"Blazing Fast Pairwise Cosine Similarity\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:url\",\"content\":\"https://joelhuang.dev/blog/blazing-fast-pairwise-cosine-similarity\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:image\",\"content\":\"https://joelhuang.dev/og?title=Blazing%20Fast%20Pairwise%20Cosine%20Similarity\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"9\",{\"property\":\"article:published_time\",\"content\":\"2021-12-18\"}],[\"$\",\"meta\",\"10\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:title\",\"content\":\"Blazing Fast Pairwise Cosine Similarity\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:image\",\"content\":\"https://joelhuang.dev/og?title=Blazing%20Fast%20Pairwise%20Cosine%20Similarity\"}],[\"$\",\"link\",\"13\",{\"rel\":\"icon\",\"href\":\"/icon.ico?76fc8b9a30e3efac\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}]]\n5:null\n"])</script></body></html>