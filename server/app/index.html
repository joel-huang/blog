<!DOCTYPE html><html lang="en" class="h-full w-full !cursor-default __className_d65c78"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/a34f9d1faa5f3315-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/a6f8c6b7131e657a-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/8376597136a899fb.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/8c26d386e0f6a889.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/cffd52bf4f3a8ac2.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-47878530f9709059.js"/><script src="/_next/static/chunks/7f586deb-6ed2ce9ddc251b73.js" async=""></script><script src="/_next/static/chunks/104-5152badf008c5e5b.js" async=""></script><script src="/_next/static/chunks/main-app-c07a9c2c909771ab.js" async=""></script><script src="/_next/static/chunks/522-d1feed2a8dc6bdb8.js" async=""></script><script src="/_next/static/chunks/709-2772e7dce5e6a6e1.js" async=""></script><script src="/_next/static/chunks/815-b72fe04fcceb75ca.js" async=""></script><script src="/_next/static/chunks/app/page-236d86b9169f16be.js" async=""></script><title>Joel Huang</title><meta name="description" content="Joel Huang"/><meta name="robots" content="index, follow"/><meta name="googlebot" content="index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"/><meta property="og:title" content="Joel Huang"/><meta property="og:description" content="Joel Huang"/><meta property="og:url" content="https://joelhuang.dev"/><meta property="og:site_name" content="Joel Huang"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Joel Huang"/><meta name="twitter:description" content="Joel Huang"/><link rel="icon" href="/icon.ico?76fc8b9a30e3efac" type="image/x-icon" sizes="16x16"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="antialiased max-w-xl mx-4 mt-8 sm:mx-auto"><main class="flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0"><aside class="-ml-[8px] mb-16 tracking-tight"><div class="lg:sticky lg:top-20"><nav class="flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative" id="nav"><div class="flex flex-row space-x-0 pr-10"><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/">about</a><a class="transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1" href="/blog">blog</a></div></nav></div></aside><section class="flex flex-col gap-4"><h1 class="text-2xl tracking-tighter flex gap-1">Joel Huang</h1><article class="prose"><h2 id="about">About</h2>
<p>I grew up creating.</p>
<p>I spent my younger years making stuff: publishing blog themes, modding games, and
running experimental MMO servers for friends to play games to experience content
they otherwise wouldn&#x27;t be able to. Those years shaped my understanding of
creation as a tool for expression and connection.</p>
<h2 id="career">Career</h2>
<p>Academia offered its intellectual allure, but the gap between theory and creation
proved too wide. I returned to my craft with a sharper mind and a mission to build
great things people love.</p>
<p>I now shape AI and product at Bifrost. Our mission it to make it incredibly simple to
create synthetic data for physical AI development. Building these tools isn&#x27;t just
good business - they&#x27;re essential for accelerating the next generation of intelligent
systems.</p>
<h2 id="contact">Contact</h2>
<p>For professional matters (consulting, collaborations, talks, workshops, hiring,
etc) - You&#x27;re welcome to reach out on
<a target="_blank" rel="noopener noreferrer" class="w-fit h-2 inline-block items-center" href="https://linkedin.com/in/joel-huang"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right inline-block"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg>LinkedIn</a> or
<a target="_blank" rel="noopener noreferrer" class="w-fit h-2 inline-block items-center" href="https://x.com/jowlz_ai"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right inline-block"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg>Twitter</a>.</p>
<p>For personal matters (mentorship,
coffee chats), do reach me via <a target="_blank" rel="noopener noreferrer" class="w-fit h-2 inline-block items-center" href="https://t.me/jowlz"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right inline-block"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg>Telegram</a> or
<a target="_blank" rel="noopener noreferrer" class="w-fit h-2 inline-block items-center" href="mailto:huang.joel@hotmail.com"><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-arrow-up-right inline-block"><path d="M7 7h10v10"></path><path d="M7 17 17 7"></path></svg>email</a>.</p></article><div><div class="flex justify-end gap-2 mb-6 text-xs"><h2 class="text-neutral-300">Filter tagged</h2><div class="flex gap-1"><button class="bg-neutral-400 text-neutral-800 elevated rounded-sm px-1 __className_262b13">all</button><button class="bg-neutral-800 text-neutral-400 elevated rounded-sm px-1 __className_262b13">ai</button></div></div><div class="w-full flex flex-col gap-2 bg-neutral-50 dark:bg-neutral-900 rounded-lg overflow-x-auto elevated p-4 pb-6"><h2 class="text-lg font-semibold tracking-tight">Articles</h2><a class="flex flex-col space-y-1" href="/blog/the-golden-hour-of-craiyon-dalle-mini"><div class="grid grid-cols-3 gap-2"><p class="text-neutral-600 dark:text-neutral-400">February 13, 2025</p><p class="col-span-2 text-neutral-900 dark:text-neutral-200 dark:hover:text-neutral-100 tracking-tight">The Golden Hour of Craiyon, or DALLÂ·E Mini</p></div></a><a class="flex flex-col space-y-1" href="/blog/blazing-fast-pairwise-cosine-similarity"><div class="grid grid-cols-3 gap-2"><p class="text-neutral-600 dark:text-neutral-400">December 18, 2021</p><p class="col-span-2 text-neutral-900 dark:text-neutral-200 dark:hover:text-neutral-100 tracking-tight">Blazing Fast Pairwise Cosine Similarity</p></div></a></div></div></section><footer class="mb-16"><p class="mt-8 text-neutral-600 dark:text-neutral-600">Â© <!-- -->2025<!-- --> Joel Huang</p></footer></main><script src="/_next/static/chunks/webpack-47878530f9709059.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/a34f9d1faa5f3315-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/media/a6f8c6b7131e657a-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n3:HL[\"/_next/static/css/8376597136a899fb.css\",\"style\"]\n4:HL[\"/_next/static/css/8c26d386e0f6a889.css\",\"style\"]\n5:HL[\"/_next/static/css/cffd52bf4f3a8ac2.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"6:I[10242,[],\"\"]\n9:I[80490,[\"522\",\"static/chunks/522-d1feed2a8dc6bdb8.js\",\"709\",\"static/chunks/709-2772e7dce5e6a6e1.js\",\"815\",\"static/chunks/815-b72fe04fcceb75ca.js\",\"931\",\"static/chunks/app/page-236d86b9169f16be.js\"],\"AllPosts\"]\nc:I[28522,[\"522\",\"static/chunks/522-d1feed2a8dc6bdb8.js\",\"709\",\"static/chunks/709-2772e7dce5e6a6e1.js\",\"815\",\"static/chunks/815-b72fe04fcceb75ca.js\",\"931\",\"static/chunks/app/page-236d86b9169f16be.js\"],\"\"]\nd:I[48071,[],\"\"]\ne:I[58073,[],\"\"]\n10:I[47832,[],\"\"]\na:T23cc,"])</script><script>self.__next_f.push([1,"# I accidentally implemented the fastest pairwise cosine similarity function\n\n\u003cTimingChart /\u003e\n\nWhile searching for a way to efficiently compute pairwise cosine similarity between vectors, I created a simple and efficient implementation using PyTorch. The function runs blazingly fast. It is faster than the popular `cosine_similarity` function from `sklearn` and the naive loop-based implementations.\n\n```\ndef pairwise_cosine_similarity(tensor: Tensor) -\u003e Tensor:\n    \"\"\"\n    Args:\n        tensor: A tensor of shape (N, D) where N is the number of\n        vectors and D is the dimensionality of the vectors.\n    Returns:\n        A tensor of shape (N, N) containing the cosine similarity\n        between all pairs of vectors.\n    \"\"\"\n\n    tmm = torch.mm(tensor, tensor.T)\n    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\n    denom_mat = torch.mm(denom.T, denom)\n    return torch.nan_to_num(tmm / denom_mat)\n```\n\n## About cosine similarity\nCosine similarity is a intuitive metric to measure similarity between two vectors. It is widely used in vision, recommendation systems, search engines, and natural language processing. The cosine similarity between two vectors is defined as the cosine of the angle between them, ranging from -1 to 1, where 1 means the vectors are identical, -1 means they are opposite, and 0 means they are orthogonal.\n\nEdit (March 2024): Be cautious about using cosine similarity when working with embeddings. Please check out [Is Cosine-Similarity of Embeddings Really About\nSimilarity?](https://arxiv.org/pdf/2403.05440.pdf)\n\n## Why pairwise similarity is slow\nWe usually want to compute the cosine similarity between all pairs of vectors. This enables do things like searching for similar vectors (similar images), analyzing the structure of the vector space (clustering images). However, with a large matrix, computing the cosine similarity can be computationally expensive.\n\nWe often find ourselves having a matrix of shape $(N, D)$ where $N$ is the number of vectors and $D$ is the dimensionality of the vectors. $D$ is also called the embedding size or dimension, which is typically a power of 2, e.g. 64, 512, 1024, 4096, etc.\n\n## The method\nThe method is based on the following formula:\n\n\u003ccenter\u003e\n$$\\text{sim}(v_i, v_j) = \\displaystyle\\frac{v_i \\cdot v_j}{\\lVert v_i \\rVert \\lVert v_j \\rVert}$$\n\u003c/center\u003e\n\nLet's explain what the code does:\n```\ntmm = torch.mm(tensor, tensor.T)\ndenom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\ndenom_mat = torch.mm(denom.T, denom)\nreturn torch.nan_to_num(tmm / denom_mat)\n```\n1. Numerator matrix: Dot product via matrix multiplication\n\n    `tmm = torch.mm(tensor, tensor.T)`\n\n    \u003e This line computes the matrix multiplication of the input `tensor` with its transpose `tensor.T`. The result is a tensor `tmm` of shape $(N, N)$ where each element $(i, j)$ represents the dot product of vectors $i$ and $j$.\n\n2. Denominator values: Norm of each vector\n\n    `denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)`\n\n    \u003e The diagonal of the tensor `tmm` contains the dot products of each vector with itself. Taking the square root of these values gives the magnitude (or norm) of each vector. The `unsqueeze(0)` function is used to add an extra dimension to the tensor, changing its shape from $(N,)$ to $(1, N)$.\n\n3. Denominator matrix\n\n    `denom_mat = torch.mm(denom.T, denom)`\n\n    \u003e This line computes the outer product of the vector `denom` with itself, resulting in a matrix `denom_mat` of shape $(N, N)$. Each element $(i, j)$ of this matrix is the product of the magnitudes of vectors $i$ and $j$.\n\n4. NaN removal\n\n    `return torch.nan_to_num(tmm / denom_mat)`\n\n    \u003e Finally, the cosine similarity between each pair of vectors is calculated by dividing the dot product matrix `tmm` by the matrix `denom_mat`. The division is element-wise, so each element $(i, j)$ of the resulting matrix represents the cosine similarity between vectors $i$ and $j$. `torch.nan_to_num` is used to replace any NaN values that might occur during the division with zeros.\n\nThe output is a symmetric matrix where the diagonal elements are all 1 (since the cosine similarity of a vector with itself is always 1), and the off-diagonal elements represent the cosine similarity between different pairs of vectors!\n\n## Benchmarking\nVersus naive loops, our approach completely outperforms them by several orders of magnitude. Versus `sklearn.metrics.pairwise.cosine_similarity`, our implementation is 10x faster, and versus a `numpy` implementation using the exact same logic, our PyTorch code is about 2-3x faster.\n\n### Results\n```\nResult within 1e-8 of scipy loop: True\nResult within 1e-8 of numpy loop: True\nResult within 1e-8 of torch loop: True\nResult within 1e-8 of sklearn: True\nResult within 1e-8 of numpy matrix: True\nscipy loop:   142362.0 us\nnumpy loop:   112752.9 us\ntorch loop:   83144.2 us\nsklearn:      401.8 us\nnumpy matrix: 136.2 us\nâœ¨ ours:      48.6 us\n```\n### Benchmark code\n\n\u003cdetails closed=\"true\"\u003e\n\u003csummary\u003eExpand to see the benchmark code used to test the functions.\u003c/summary\u003e\n```\nimport torch\nimport numpy as np\nimport scipy\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndef torch_loop_cosine_similarity(tensor, output):\n    for i in range(len(tensor)):\n        for j in range(len(tensor)):\n            output[i, j] = torch.cosine_similarity(\n                tensor[i].unsqueeze(0), tensor[j].unsqueeze(0)\n            )\n    return output\n\n\ndef scipy_loop_cosine_similarity(tensor, output):\n    for i in range(len(tensor)):\n        for j in range(len(tensor)):\n            output[i, j] = scipy.spatial.distance.cosine(tensor[i], tensor[j])\n    return output\n\n\ndef numpy_loop_cosine_similarity(tensor, output):\n    for i in range(len(tensor)):\n        for j in range(len(tensor)):\n            output[i, j] = float(\n                np.dot(tensor[i], tensor[j])\n                / (np.linalg.norm(tensor[i]) * np.linalg.norm(tensor[j]))\n            )\n    return output\n\n\ndef numpy_matrix_cosine_similarity(tensor):\n    tmm = np.matmul(tensor, tensor.T)\n    denom = np.sqrt(tmm.diagonal()).unsqueeze(0)\n    denom_mat = np.matmul(denom.T, denom)\n    return np.nan_to_num(tmm / denom_mat)\n\n\ndef torch_matrix_cosine_similarity(tensor):\n    tmm = torch.mm(tensor, tensor.T)\n    denom = torch.sqrt(tmm.diagonal()).unsqueeze(0)\n    denom_mat = torch.mm(denom.T, denom)\n    return torch.nan_to_num(tmm / denom_mat)\n\n\nif __name__ == \"__main__\":\n\n    # Create a random data tensor of shape (N=50, D=64)\n    data = torch.rand((50, 64))\n\n    # Create the output tensor\n    zeros = torch.zeros((data.shape[0], data.shape[0]))\n\n    # Compare across different implementations\n    sklearn = torch.Tensor(cosine_similarity(data, data))\n    npy_matrix = torch.Tensor(numpy_matrix_cosine_similarity(data))\n    npy_loop = numpy_loop_cosine_similarity(data, zeros)\n    spy_loop = scipy_loop_cosine_similarity(data, zeros)\n    torch_loop = torch_loop_cosine_similarity(data, zeros)\n    ours = torch_matrix_cosine_similarity(data)\n\n    print(f\"Result within 1e-8 of scipy loop: {bool(torch.allclose(ours, spy_loop))}\")\n    print(f\"Result within 1e-8 of numpy loop: {bool(torch.allclose(ours, npy_loop))}\")\n    print(f\"Result within 1e-8 of torch loop: {bool(torch.allclose(ours, torch_loop))}\")\n    print(f\"Result within 1e-8 of sklearn: {bool(torch.allclose(ours, sklearn))}\")\n    print(\n        f\"Result within 1e-8 of numpy matrix: {bool(torch.allclose(ours, npy_matrix))}\"\n    )\n\n    import timeit\n\n    t0 = timeit.Timer(\n        stmt=\"torch_loop_cosine_similarity(data, output)\",\n        setup=\"from __main__ import torch_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\n        globals={\"data\": data},\n    )\n\n    t1 = timeit.Timer(\n        stmt=\"cosine_similarity(data, data)\",\n        setup=\"from __main__ import cosine_similarity\",\n        globals={\"data\": data},\n    )\n\n    t2 = timeit.Timer(\n        stmt=\"scipy_loop_cosine_similarity(data, output)\",\n        setup=\"from __main__ import scipy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\n        globals={\"data\": data},\n    )\n\n    t5 = timeit.Timer(\n        stmt=\"numpy_loop_cosine_similarity(data, output)\",\n        setup=\"from __main__ import numpy_loop_cosine_similarity; import torch; output = torch.zeros((data.shape[0], data.shape[0]));\",\n        globals={\"data\": data},\n    )\n\n    t3 = timeit.Timer(\n        stmt=\"numpy_matrix_cosine_similarity(data)\",\n        setup=\"from __main__ import numpy_matrix_cosine_similarity\",\n        globals={\"data\": data},\n    )\n\n    t4 = timeit.Timer(\n        stmt=\"torch_matrix_cosine_similarity(data)\",\n        setup=\"from __main__ import torch_matrix_cosine_similarity\",\n        globals={\"data\": data},\n    )\n\n    print(f\"scipy loop:   {t2.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\n    print(f\"numpy loop:   {t5.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\n    print(f\"torch loop:   {t0.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\n    print(f\"sklearn:      {t1.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\n    print(f\"numpy matrix: {t3.timeit(100) / 100 * 1e6:\u003e5.1f} us\")\n    print(f\"âœ¨ ours:     {t4.timeit(10) / 100 * 1e6:\u003e5.1f} us\")\n```\n\u003c/details\u003e"])</script><script>self.__next_f.push([1,"b:T1116,"])</script><script>self.__next_f.push([1,"When future humans look back at the development of AI, some moments will\nstand out as historic.\n\nThey'll argue about which moment was more legendary:\nthe invention of backpropagation! No, it's AlexNet ushering in deep learning.\nWho could forget *Transformers is All You Need*? Surely it must be AlphaGo\nbeating Lee Sedol. Midjourney's #1 discord server. ChatGPT.\n\nBut a few fleeting moments - a little less well acknowledged,\na whole lot less significant - will dominate the memories of a few\nblessed to have experienced their sheer weirdness.\n\nThe short-lived era of DALLÂ·E Mini, later rechristened as Craiyon, is such\na moment. A fugitive golden hour when AI-generated art existed in a beautiful\nstate of grotesqueness, democratized access, and unbridled experimentation.\n\n\u003cTimeline /\u003e\n\n## Just good enough to slap a UI on\n\nDALLÂ·E Mini was an unexpected protagonist in the AI \"art\" revolution, not\nbecause it was good, but because it was first, free, and functional.\n\nWhile more sophisticated models remained behind institutional (fire)walls\namidst a period of \"too dangerous to release\" crisis narratives, artist\nBoris Dayma decided to toss a simple text box and a little orange button\non a website, inviting anyone with an internet connection and a minute to\nparticipate in his grand little experiment. Sam Altman would do well to\ntake a leaf out of Boris's book.\n\nThe results were gloriously absurd. Users delighted in prompting the system with\njuicy scenarios, waiting a few minutes each time to roll 9 images in its signature\n3x3 grid:\n\n\u003cCraiyonExamples /\u003e\n\nDALLÂ·E Mini's tendency to produce warped faces, melted hands, and inexplicable artifacts\nbecame features rather than bugs, birthing a cursed aesthetic vocabulary I and many\ncame to love. The images were so fucking bad they were good.\n\n## Social experimentation\n\nThe imperfections in these early models created space for human understanding, meaning-making,\nand meme-making.\n\nPeople used biases present in the models to make jokes about politicians, fictional characters,\nand pop culture. Absurdist stonks dominated this meme exchange (when anything is possible, most\nthings are absurd). \n\nEvery time someone spent a minute only to end up with gross human hands or faces, pure black images,\nor horrifying repeated patterns, they built a small bit of intuition for how these things are made.\n\nPeople quickly figured out tricks, like saying \"unreal engine\" to boost realism or exploiting glitch\ntokens to peek into the model's latent space. These hacks spread fast, fueling further exploration\nand vibrant discourse. It felt sort of like the early days of the internet, or going to college with\na bunch of weirdos (in the best way possible).\n\n## Consciously preserving space for the weird and wonderful\n\nToday's image and video generators are already powering early adopters and the next wave of creators.\nFlux 1.1 Pro vs. Midjourney v6.1 dominates the conversation today, and the world is better for it.\nProgress and commercialization are inevitable and undeniably valuable, and we see clear paths to\nproductivity across visually creative domains.\n\nThe sun set long ago on DALLÂ·E Mini as a cutting-edge research demo. Yet its imperfections reveal\nsomething more truthful than polish ever could. In the pursuit of faster, better, cheaper - find\nbeauty in the short-lived spaces between.\n\n\u003cdiv className=\"flex items-center justify-center\"\u003e\n  \u003cImage src=\"/duolingo-trail.webp\" width={400} height={400} /\u003e\n\u003c/div\u003e\n\n\u003cdiv className=\"mt-8 elevated p-4 bg-neutral-800 rounded-lg\"\u003e\n\u003cspan className=\"my-2\"\u003e\n  {\"PS: Although Craiyon has long moved on to newer models, the DALLÂ·E Mini experience is immortalized here: \"}\n  \u003ca href=\"https://huggingface.co/spaces/dalle-mini/dalle-mini\"\u003eðŸ¤— DALLÂ·E Mini on HF Spaces\u003c/a\u003e{\".\"}\n\u003c/span\u003e\n\u003c/div\u003e\n\n## Further reading\n\n\u003cdiv className=\"flex flex-col gap-1\"\u003e\n[Wired - DALL-E Mini Is the Internet's Favorite AI Meme Machine](https://www.wired.com/story/dalle-ai-meme-machine/)\n[DALL-E Mini Explained (W\u0026B tech report)](https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained--Vmlldzo4NjIxODA)\n[Making Moves in DALLÂ·E Mini](https://thejaymo.net/2022/06/19/250-making-moves-in-dalle-mini/)\n[Unpopular opinion: the rise of dalle mini has destroyed chances of this...](https://www.reddit.com/r/dalle2/comments/vbqzbp/unpopular_opinion_the_rise_of_dalle_mini_has/)\n\u003c/div\u003e"])</script><script>self.__next_f.push([1,"11:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L6\",null,{\"buildId\":\"7Zr-qunAZlU_ddQ3Hcf6Q\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"\"],\"initialTree\":[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"__PAGE__\",{},[[\"$L7\",[\"$\",\"section\",null,{\"className\":\"flex flex-col gap-4\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-2xl tracking-tighter flex gap-1\",\"children\":\"Joel Huang\"}],[\"$\",\"article\",null,{\"className\":\"prose\",\"children\":\"$L8\"}],[\"$\",\"$L9\",null,{\"posts\":[{\"metadata\":{\"title\":\"Blazing Fast Pairwise Cosine Similarity\",\"publishedAt\":\"2021-12-18\",\"tags\":[\"ai\"]},\"slug\":\"blazing-fast-pairwise-cosine-similarity\",\"content\":\"$a\"},{\"metadata\":{\"title\":\"The Golden Hour of Craiyon, or DALLÂ·E Mini\",\"publishedAt\":\"2025-02-13\",\"tags\":[\"ai\"]},\"slug\":\"the-golden-hour-of-craiyon-dalle-mini\",\"content\":\"$b\"}],\"tags\":[\"all\",\"ai\"]}]]}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/cffd52bf4f3a8ac2.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]]],null],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8376597136a899fb.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8c26d386e0f6a889.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"h-full w-full !cursor-default __className_d65c78\",\"children\":[\"$\",\"body\",null,{\"className\":\"antialiased max-w-xl mx-4 mt-8 sm:mx-auto\",\"children\":[\"$\",\"main\",null,{\"className\":\"flex-auto min-w-0 mt-6 flex flex-col px-2 md:px-0\",\"children\":[[\"$\",\"aside\",null,{\"className\":\"-ml-[8px] mb-16 tracking-tight\",\"children\":[\"$\",\"div\",null,{\"className\":\"lg:sticky lg:top-20\",\"children\":[\"$\",\"nav\",null,{\"className\":\"flex flex-row items-start relative px-0 pb-0 fade md:overflow-auto scroll-pr-6 md:relative\",\"id\":\"nav\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-row space-x-0 pr-10\",\"children\":[[\"$\",\"$Lc\",\"/\",{\"href\":\"/\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"about\"}],[\"$\",\"$Lc\",\"/blog\",{\"href\":\"/blog\",\"className\":\"transition-all hover:text-neutral-800 dark:hover:text-neutral-200 flex align-middle relative py-1 px-2 m-1\",\"children\":\"blog\"}]]}]}]}]}],[\"$\",\"$Ld\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$Le\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$\",\"section\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"mb-8 text-2xl font-semibold tracking-tighter\",\"children\":\"404 - Page Not Found\"}],[\"$\",\"p\",null,{\"className\":\"mb-4\",\"children\":\"The page you are looking for does not exist.\"}]]}],\"notFoundStyles\":[]}],[\"$\",\"footer\",null,{\"className\":\"mb-16\",\"children\":[\"$\",\"p\",null,{\"className\":\"mt-8 text-neutral-600 dark:text-neutral-600\",\"children\":[\"Â© \",2025,\" Joel Huang\"]}]}]]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lf\"],\"globalErrorComponent\":\"$10\",\"missingSlots\":\"$W11\"}]\n"])</script><script>self.__next_f.push([1,"8:[[\"$\",\"h2\",null,{\"id\":\"about\",\"children\":\"About\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I grew up creating.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I spent my younger years making stuff: publishing blog themes, modding games, and\\nrunning experimental MMO servers for friends to play games to experience content\\nthey otherwise wouldn't be able to. Those years shaped my understanding of\\ncreation as a tool for expression and connection.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"career\",\"children\":\"Career\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Academia offered its intellectual allure, but the gap between theory and creation\\nproved too wide. I returned to my craft with a sharper mind and a mission to build\\ngreat things people love.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I now shape AI and product at Bifrost. Our mission it to make it incredibly simple to\\ncreate synthetic data for physical AI development. Building these tools isn't just\\ngood business - they're essential for accelerating the next generation of intelligent\\nsystems.\"}],\"\\n\",[\"$\",\"h2\",null,{\"id\":\"contact\",\"children\":\"Contact\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"For professional matters (consulting, collaborations, talks, workshops, hiring,\\netc) - You're welcome to reach out on\\n\",[\"$\",\"$Lc\",null,{\"href\":\"https://linkedin.com/in/joel-huang\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"w-fit h-2 inline-block items-center\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":16,\"height\":16,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right inline-block\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}],\"LinkedIn\"]}],\" or\\n\",[\"$\",\"$Lc\",null,{\"href\":\"https://x.com/jowlz_ai\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"w-fit h-2 inline-block items-center\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":16,\"height\":16,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right inline-block\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}],\"Twitter\"]}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"For personal matters (mentorship,\\ncoffee chats), do reach me via \",[\"$\",\"$Lc\",null,{\"href\":\"https://t.me/jowlz\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"w-fit h-2 inline-block items-center\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":16,\"height\":16,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right inline-block\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}],\"Telegram\"]}],\" or\\n\",[\"$\",\"$Lc\",null,{\"href\":\"mailto:huang.joel@hotmail.com\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"w-fit h-2 inline-block items-center\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":16,\"height\":16,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"lucide lucide-arrow-up-right inline-block\",\"children\":[[\"$\",\"path\",\"1tivn9\",{\"d\":\"M7 7h10v10\"}],[\"$\",\"path\",\"1vkiza\",{\"d\":\"M7 17 17 7\"}],\"$undefined\"]}],\"email\"]}],\".\"]}]]\n"])</script><script>self.__next_f.push([1,"f:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Joel Huang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Joel Huang\"}],[\"$\",\"meta\",\"4\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"5\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Joel Huang\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Joel Huang\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://joelhuang.dev\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"Joel Huang\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:title\",\"content\":\"Joel Huang\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:description\",\"content\":\"Joel Huang\"}],[\"$\",\"link\",\"15\",{\"rel\":\"icon\",\"href\":\"/icon.ico?76fc8b9a30e3efac\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"meta\",\"16\",{\"name\":\"next-size-adjust\"}]]\n7:null\n"])</script></body></html>